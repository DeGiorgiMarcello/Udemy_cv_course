{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "## Template Matching\n",
    "We have a template that we want to find in a bigger image. The base idea is to take this template and slide it on the whole image finding a match, if any.\n",
    "\n",
    "### MiniProject - Finding Waldo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "path = 'images/cap5/'\n",
    "\n",
    "def show_image(img,title=\"img\",waitKey=True):\n",
    "    cv2.imshow(title,img)\n",
    "    if waitKey:\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(path+'waldo.jpg')\n",
    "img = cv2.resize(img,None,fx=0.6,fy=0.6)\n",
    "template = cv2.imread(path+'waldo_template.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "template = cv2.resize(template,None,fx=0.6,fy=0.6)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "show_image(img,\"Where is Waldo?\")\n",
    "\n",
    "result = cv2.matchTemplate(gray,template,cv2.TM_CCOEFF)\n",
    "min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "#create bounding box\n",
    "top_left = max_loc\n",
    "bottom_right = (top_left[0]+50,top_left[1]+50)\n",
    "cv2.rectangle(img,top_left,bottom_right,(0,255,0),3)\n",
    "\n",
    "show_image(img,\"...here!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method here used is the **TM_CCOEFF**, correlation coefficient. This method slides the template on the image and for each pixel area computes the correlation coefficients to determine how good or how bad is the matching.\n",
    "With **minMaxLoc** we take the coordinates of the area with the best correlation coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is not so good. It is NOT indipendent on scaling, rotation, photometric changes (light, contrast etc), distortion in the point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding corners\n",
    "We use the **cv2.cornerHarris** algorithm. The parameters are:\n",
    "\n",
    "\n",
    "- img - Input image, it should be grayscale and float32 type.\n",
    "- blockSize - It is the size of neighbourhood considered for corner detection\n",
    "- ksize - Aperture parameter of Sobel derivative used.\n",
    "- k - Harris detector free parameter in the equation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(path+'chessboard.jpg')\n",
    "copy = np.copy(img)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray = gray.astype(np.float32)\n",
    "harris_corners = cv2.cornerHarris(gray,3,3,0.05)\n",
    "\n",
    "#let's use dilation to enlarge the corners for a bettere visualization\n",
    "kernel = np.ones((7,7),np.uint8)\n",
    "harris_corners = cv2.dilate(harris_corners,kernel,iterations=2)\n",
    "#let's mask the original image\n",
    "copy[harris_corners > 0.025*harris_corners.max()] = (0,255,0)\n",
    "show_image(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved feature detection using \"Good Features to Track\"\n",
    "These are the parameter used:\n",
    "- image – Input 8-bit or floating-point 32-bit, single-channel image\n",
    "- maxCorners – Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned.\n",
    "- qualityLevel – Parameter characterizing the minimal accepted quality of image corners\n",
    "- minDistance – Minimum possible Euclidean distance between the returned corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = np.copy(img)\n",
    "#we get the first top 50 corners\n",
    "corners = cv2.goodFeaturesToTrack(gray,50,0.01,15)\n",
    "for corner in corners:\n",
    "    x,y = corner[0]\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    cv2.rectangle(copy,(x-10,y-10),(x+10,y+10),(0,255,0),2)\n",
    "show_image(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some considerations about corners...\n",
    "Corners **are tolerant** of rotations,translations and slight photometric changes BUT are **NOT tolerant** of scaling and large changes in brightness, contrast etc.\n",
    "To make it tolerant to scaling we should shrink or enlarge the detector window as well.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT, SURF, FAST, BRIEF and ORB\n",
    "\n",
    "All of these are algorithms used for detect keypoints. These are different for speed, complexity, good results. SIFT and SURF are now patented so we can't use them with the latest version of opencv. We need opencv version '3.4.2.16'. \n",
    "\n",
    "### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keyPoints detected:  2620\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(path+'bologna.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "keyPoints = sift.detect(gray,None)\n",
    "print(\"Number of keyPoints detected: \",len(keyPoints))\n",
    "out = np.copy(img)\n",
    "cv2.drawKeypoints(img,keyPoints,out,flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_image(out,\"SIFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keyPoints detected:  2646\n"
     ]
    }
   ],
   "source": [
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "keyPoints = surf.detect(gray,None)\n",
    "print(\"Number of keyPoints detected: \",len(keyPoints))\n",
    "out = np.copy(img)\n",
    "cv2.drawKeypoints(img,keyPoints,out,flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_image(out,\"SURF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keyPoints detected:  9020\n"
     ]
    }
   ],
   "source": [
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "#Obtain the keyPoints. NonMaxSuppression is On by default\n",
    "keyPoints = fast.detect(gray,None)\n",
    "print(\"Number of keyPoints detected: \",len(keyPoints))\n",
    "out = np.copy(img)\n",
    "cv2.drawKeypoints(img,keyPoints,out,flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_image(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRIEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keyPoints detected:  7862\n"
     ]
    }
   ],
   "source": [
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "#we have to use fast to get the keypoints and then apply them to brief\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "keyPoints = fast.detect(gray,None)\n",
    "keyPoints,descriptors = brief.compute(gray,keyPoints)\n",
    "print(\"Number of keyPoints detected: \",len(keyPoints))\n",
    "out = np.copy(img)\n",
    "cv2.drawKeypoints(img,keyPoints,out,flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_image(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keyPoints detected:  500\n"
     ]
    }
   ],
   "source": [
    "#create an ORB object and define the max number of keypoints\n",
    "orb = cv2.ORB_create(5000)\n",
    "keyPoints = orb.detect(gray,None)\n",
    "print(\"Number of keyPoints detected: \",len(keyPoints))\n",
    "out = np.copy(img)\n",
    "cv2.drawKeypoints(img,keyPoints,out,flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_image(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
